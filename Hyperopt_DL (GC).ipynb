{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hyperopt_DL (GC).ipynb","provenance":[],"collapsed_sections":["v0yRQ_ZSMaFG","Xx3FVtgkOgyq"],"machine_shape":"hm","authorship_tag":"ABX9TyNMMfNmtF+Uqczi+Pyu7kIr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HIoNaM-5rkU6"},"source":["**Neural network experiment**"]},{"cell_type":"markdown","metadata":{"id":"4SGhW_-Gygr-"},"source":["# **import user-specified packages and google drive files**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUdXFuf774wi","executionInfo":{"status":"ok","timestamp":1638802529391,"user_tz":-60,"elapsed":12,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}},"outputId":"55a5853d-cbf8-4eca-b294-28303791259b"},"source":["%xmode Verbose"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Exception reporting mode: Verbose\n"]}]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":72},"id":"JaEJuClC1bmA","executionInfo":{"status":"ok","timestamp":1638802552161,"user_tz":-60,"elapsed":18341,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}},"outputId":"c79e9fe3-7907-42aa-d5b4-2c24c038034b"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-f7fadf2a-314a-44ef-9586-c679ec45f1ad\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f7fadf2a-314a-44ef-9586-c679ec45f1ad\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving dataset_confs.py to dataset_confs.py\n","Saving DatasetManager.py to DatasetManager.py\n"]}]},{"cell_type":"code","metadata":{"id":"0HYLYcjC1bmB","executionInfo":{"status":"ok","timestamp":1638802552660,"user_tz":-60,"elapsed":502,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}}},"source":["import dataset_confs\n","from DatasetManager import DatasetManager"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMxd6y3fIACe"},"source":["# **import datasets**"]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":107},"id":"AyGegYIGJ_1S","executionInfo":{"status":"ok","timestamp":1638803053906,"user_tz":-60,"elapsed":498126,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}},"outputId":"4638e9e5-9fa0-4399-dcd3-aad1b6a2cb7b"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-91703622-7d08-49a0-822e-64ee3f489609\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-91703622-7d08-49a0-822e-64ee3f489609\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving BPIC11_f1.csv to BPIC11_f1.csv\n","Saving BPIC11_f2.csv to BPIC11_f2.csv\n","Saving BPIC11_f3.csv to BPIC11_f3.csv\n","Saving BPIC11_f4.csv to BPIC11_f4.csv\n"]}]},{"cell_type":"code","metadata":{"id":"t9mASjC4ziWt","executionInfo":{"status":"ok","timestamp":1638803536879,"user_tz":-60,"elapsed":280,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}}},"source":["csv_files = {\n","    \"bpic2011\": [\"BPIC11_f%s\"%formula for formula in range(1,5)],\n","    #\"bpic2015\": [\"BPIC15_%s_f2\"%(municipality) for municipality in range(3,6)],\n","    #\"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"],\n","    #\"bpic2012\": [\"bpic2012_O_ACCEPTED-COMPLETE\",\"bpic2012_O_CANCELLED-COMPLETE\",\"bpic2012_0_DECLINED-COMPLETE\"],\n","    #\"production\": [\"Production\"],\n","    #\"bpic2017\": [\"BPIC17_O_Accepted\",\"BPIC17_O_Cancelled\",\"BPIC17_0_Refused\"],\n","    #\"traffic_fines\": [\"traffic_fines_%s\"%formula for formula in range(1,3)],\n","    #\"hospital_billing\": [\"hospital_billing_%s\"%suffix for suffix in [2,3]]\n","}\n","files = []\n","for k, v in csv_files.items():\n","    files.extend(v)\n","dataset_ref_to_datasets = {\n","     \"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n","    #\"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(3,6)],\n","    #\"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"],\n","    #\"bpic2012\": [\"bpic2012_accepted\",\"bpic2012_cancelled\",\"bpic2012_declined\"],\n","    #\"production\": [\"production\"],\n","    #\"bpic2017\": [\"bpic2017_accepted\",\"bpic2017_cancelled\",\"bpic2017_refused\"],\n","    #\"traffic_fines\": [\"traffic_fines_%s\"%formula for formula in range(1,3)],\n","    #\"hospital_billing\": [\"hospital_billing_%s\"%suffix for suffix in [2,3]]\n","}\n","\n","files = []\n","for k, v in csv_files.items():\n","    files.extend(v)\n","datasets = []\n","for k, v in dataset_ref_to_datasets.items():\n","    datasets.extend(v)\n","res = {datasets[i]: files[i] for i in range(len(datasets))}"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzz7ApNGUvsA","executionInfo":{"status":"ok","timestamp":1638803538770,"user_tz":-60,"elapsed":214,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}},"outputId":"bfcf3484-d964-42a9-b08a-f6d1fb76c435"},"source":["datasets"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['bpic2011_f1', 'bpic2011_f2', 'bpic2011_f3', 'bpic2011_f4']"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kP7FNSDIOzN","executionInfo":{"status":"ok","timestamp":1638803540474,"user_tz":-60,"elapsed":596,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}},"outputId":"2ad4178c-1cbb-4150-baee-160eaaa19b97"},"source":["res"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'bpic2011_f1': 'BPIC11_f1',\n"," 'bpic2011_f2': 'BPIC11_f2',\n"," 'bpic2011_f3': 'BPIC11_f3',\n"," 'bpic2011_f4': 'BPIC11_f4'}"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"v0yRQ_ZSMaFG"},"source":["# **import packages and functions**"]},{"cell_type":"code","metadata":{"id":"uwBik9AUO-VZ","executionInfo":{"status":"ok","timestamp":1638804805158,"user_tz":-60,"elapsed":278,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}}},"source":["#Functions and packages\n","import pandas as pd\n","import numpy as np\n","import os\n","import pickle\n","import random\n","from scipy.stats import spearmanr\n","from sklearn.metrics import roc_auc_score\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from pandas.api.types import is_string_dtype\n","from collections import OrderedDict\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","\n","#hyperopt\n","import hyperopt\n","from hyperopt import hp, Trials, fmin, tpe, STATUS_OK\n","from hyperopt.pyll.base import scope\n","\n","#LSTM\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dense, Dropout, Input, Multiply, concatenate, Embedding, LSTM\n","from tensorflow.keras.layers import Bidirectional, TimeDistributed\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Nadam, Adam, SGD, RMSprop\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","import tensorflow.keras.utils as ku\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.layers import Softmax, Lambda\n","from tensorflow.keras import backend\n","\n","#CNN\n","from tensorflow.keras.layers import Conv1D"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSXBB6FWOyRN"},"source":["# **Own created functions**"]},{"cell_type":"code","metadata":{"id":"uH3LRIBHOyap","executionInfo":{"status":"ok","timestamp":1638804812410,"user_tz":-60,"elapsed":639,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}}},"source":["#functions\n","#https://towardsdatascience.com/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2\n","class ColumnEncoder(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.columns = None\n","        self.maps = dict()\n","\n","    def transform(self, X):\n","        X_copy = X.copy()\n","        for col in self.columns:\n","            # encode value x of col via dict entry self.maps[col][x]+1 if present, otherwise 0\n","            X_copy.loc[:,col] = X_copy.loc[:,col].apply(lambda x: self.maps[col].get(x, -1)+1)\n","        return X_copy\n","\n","    def inverse_transform(self, X):\n","        X_copy = X.copy()\n","        for col in self.columns:\n","            values = list(self.maps[col].keys())\n","            # find value in ordered list and map out of range values to None\n","            X_copy.loc[:,col] = [values[i-1] if 0<i<=len(values) else None for i in X_copy[col]]\n","        return X_copy\n","\n","    def fit(self, X, y=None):\n","        # only apply to string type columns\n","        self.columns = [col for col in X.columns if is_string_dtype(X[col])]\n","        for col in self.columns:\n","            self.maps[col] = OrderedDict({value: num for num, value in enumerate(sorted(set(X[col])))})\n","        return self\n","\n","def prepare_inputs(X_train, X_test, data):  \n","    global ce\n","    ce = ColumnEncoder()\n","    X_train, X_test = X_train.astype(str), X_test.astype(str)\n","    X_train_enc = ce.fit_transform(X_train)\n","    X_test_enc = ce.transform(X_test)\n","    return X_train_enc, X_test_enc\n","    \n","def numeric_padding(sequences, maxlen=None, value=0):\n","    num_samples = len(sequences)\n","    sample_shape = np.asarray(sequences[0]).shape[1:]\n","    x = np.full((num_samples, maxlen) + sample_shape, value)\n","    for idx, s in enumerate(sequences):\n","        trunc = s[:maxlen]\n","        x[idx, :maxlen] = trunc[0]\n","        \n","def create_index(log_df, column):\n","    \"\"\"Creates an idx for a categorical attribute.\n","    Args:\n","        log_df: dataframe.\n","        column: column name.\n","    Returns:\n","        index of a categorical attribute pairs.\n","    \"\"\"\n","    temp_list = log_df[[column]].values.tolist()\n","    subsec_set = {str((x[0])) for x in temp_list}\n","    subsec_set = sorted(list(subsec_set))\n","    alias = dict()\n","    for i, _ in enumerate(subsec_set):\n","        alias[subsec_set[i]] = i + 1\n","    return alias\n","\n","def groupby_caseID(data, cols):\n","    ans = [pd.DataFrame(y) for x, y in data[cols].groupby('Case ID', as_index=False)]\n","    return ans\n","\n","def remove_punctuations(columns_before):\n","    columns = []\n","    for string in columns_before:\n","        new_string = string.replace(\":\", \"_\")\n","        columns.append(new_string)\n","    return columns\n","\n","#call this function with the name of the right column\n","def create_indexes(i, data):\n","    cat_index = create_index(data, i)\n","    cat_index['Start'] = 0\n","    cat_index['End'] = len(cat_index)\n","    index_cat = {v: k for k, v in cat_index.items()}\n","    cat_weights = ku.to_categorical(sorted(index_cat.keys()), len(cat_index))\n","    no_cols = len(data.groupby([i]))+1\n","    return cat_weights, index_cat, cat_index, no_cols\n","\n","def labels_after_grouping(data_train,data_test):\n","    train_labels = []\n","    for i in range (0,len(data_train)):\n","        temp_label = data_train[i]['label'].iloc[0]\n","        train_labels.append(temp_label)\n","\n","    test_labels = []\n","    for i in range (0,len(data_test)):\n","        temp_label = data_test[i]['label'].iloc[0]\n","        test_labels.append(temp_label)\n","    train_y = [1 if i!='regular' else 0 for i in train_labels]\n","    test_y = [1 if i!='regular' else 0 for i in test_labels]\n","    return train_y, test_y\n","\n","def pad_cat_data(cols, data_train, data_test, maxlen):\n","    \n","    #padding of the different categorical columns\n","    #train paddings\n","    paddings_train = []\n","    for i in cols:\n","        padding= []\n","        for k in range(0,len(data_train)):\n","            temp = []\n","            temp = list(data_train[k][i])\n","            padding.append(temp)\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        #padded = padded/len(data.groupby([i]))\n","        paddings_train.append(padded)\n","\n","    #test paddings\n","    paddings_test = []\n","    for i in cols:\n","        padding= []\n","        for k in range(0,len(data_test)):\n","            temp = []\n","            temp = list(data_test[k][i])\n","            padding.append(temp)\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        #padded = padded/len(data.groupby([i]))\n","        paddings_test.append(padded)\n","    return paddings_train, paddings_test\n","\n","def pad_num_data(cols, data_train, data_test, maxlen, dt_train_prefixes, dt_test_prefixes):\n","    pad_train = []\n","    pad_test  = []\n","    for i in cols:\n","        \n","        padding = []\n","        for k in range(0,len(data_train)):\n","            temp_train = []\n","            temp_train = list(data_train[k][i])\n","            padding.append(temp_train)\n","\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        if dt_train_prefixes[i].max() !=0:\n","           \n","            padded = padded/dt_train_prefixes[i].max()\n","        else:\n","            padded = padded\n","        pad_train.append(padded)\n","   \n","    for i in cols:\n","      \n","        padding = []\n","        for k in range(0,len(data_test)):\n","            temp_test = []\n","            temp_test = list(data_test[k][i])\n","            padding.append(temp_test)\n","      \n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        if dt_test_prefixes[i].max() !=0:\n","            padded = padded/dt_test_prefixes[i].max()\n","        else:\n","            padded = padded\n","        pad_test.append(padded)\n","    return pad_train, pad_test\n","\n","def reshape_num_data(pad_data, cutoff):\n","        pad_num = np.reshape(pad_data, (len(pad_data), cutoff, 1))\n","        return pad_num"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xx3FVtgkOgyq"},"source":["# **parameters**"]},{"cell_type":"code","metadata":{"id":"Cnlhy3XIOgof","executionInfo":{"status":"ok","timestamp":1638804899134,"user_tz":-60,"elapsed":243,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}}},"source":["#terminology \n","#event log dictionary\n","params_dir = './params_dir_DL'\n","results_dir = './results' \n","column_selection= 'all'\n","cls_encoding ='embeddings'\n","classifiers =['LSTM','CNN']\n","n_iter = 1\n","n_splits = 3\n","train_ratio = 0.8\n","random_state = 22\n","l2reg=0.001\n","allow_negative=False\n","incl_time = True \n","incl_res = True\n","# create results directory\n","if not os.path.exists(os.path.join(params_dir)):\n","    os.makedirs(os.path.join(params_dir))"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UpCBthWhMhkS"},"source":["# **Function for preprocessing the data**"]},{"cell_type":"code","metadata":{"id":"JqpKGwadG0sH","executionInfo":{"status":"ok","timestamp":1638804843003,"user_tz":-60,"elapsed":201,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}}},"source":["# function for preprocessing data\n","\n","def create_data(dt_train_prefixes, dt_test_prefixes):\n","\n","\n","  #get the label of the train and test set\n","  test_y = dataset_manager.get_label_numeric(dt_test_prefixes)\n","  train_y = dataset_manager.get_label_numeric(dt_train_prefixes)   \n","  \n","  #cat columns integerencoded\n","  cat_cols = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['static_cat_cols']\n","\n","  dt_train_prefixes[cat_cols],dt_test_prefixes[cat_cols]= prepare_inputs(dt_train_prefixes[cat_cols], dt_test_prefixes[cat_cols], data)\n","  dt_train_prefixes[cat_cols] = dt_train_prefixes[cat_cols]+1\n","  dt_test_prefixes[cat_cols] = dt_test_prefixes[cat_cols]+1\n","  #append caseId and label\n","  cat_cols.append('Case ID')\n","  cat_cols.append('label')\n","  #groupby case ID\n","  \n","  ans_train = groupby_caseID(dt_train_prefixes, cat_cols)\n","  ans_test = groupby_caseID(dt_test_prefixes, cat_cols)\n","  #obtain the new label lists after grouping\n","  train_y, test_y = labels_after_grouping(ans_train, ans_test)\n","  #remove then back\n","  cat_cols.remove('label')\n","  cat_cols.remove('Case ID')\n","  #pad cat columns\n","  paddings_train, paddings_test = pad_cat_data(cat_cols, ans_train, ans_test, maxlen)\n","  \n","  #NUMERICAL COLUMNS\n","  numerical_columns = cls_encoder_args['dynamic_num_cols']+cls_encoder_args['static_num_cols']\n","  numerical_columns.remove('timesincelastevent')\n"," \n","  numerical_columns.append('Case ID')\n","  ans_train2 = groupby_caseID(dt_train_prefixes, numerical_columns)\n","  ans_test2 = groupby_caseID(dt_test_prefixes, numerical_columns )\n","  numerical_columns.remove('Case ID')  \n","  pad_train, pad_test = pad_num_data(numerical_columns, ans_train2, ans_test2, maxlen, dt_train_prefixes, dt_test_prefixes)\n","  \n","  #time inputs                   \n","  ans_time_train= groupby_caseID(dt_train_prefixes,['timesincelastevent', 'Case ID'])\n","  ans_time_test = groupby_caseID(dt_test_prefixes,['timesincelastevent', 'Case ID'])\n","  pad_time_train, pad_time_test = pad_num_data(['timesincelastevent'], ans_time_train, ans_time_test, maxlen, dt_train_prefixes, dt_test_prefixes)\n","  #reshape the time input\n","  padded_time = reshape_num_data(pad_time_train[0], cutoff)\n","  padded_time_test=  reshape_num_data(pad_time_test[0], cutoff)\n","            \n","  return pad_train, pad_test, paddings_train, paddings_test, padded_time, padded_time_test, train_y, test_y"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pGkGnMehMouY"},"source":["# **Create and evaluate model**"]},{"cell_type":"code","metadata":{"id":"NZBkpmopFNEa","executionInfo":{"status":"ok","timestamp":1638806537430,"user_tz":-60,"elapsed":645,"user":{"displayName":"Alexander Stevens","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioGPyY8p90bx9tcVhnxMNNrAwYuAvvK_35hSahxQ=s64","userId":"06895933540823444481"}}},"source":["def create_and_evaluate_model(args):     \n","        global trial_nr\n","        trial_nr += 1\n","        for cv_iter in range(n_splits):\n","          dt_test_prefixes_original = dt_prefixes[cv_iter]\n","          dt_train_prefixes_original = pd.DataFrame()\n","          for cv_train_iter in range(n_splits): \n","              if cv_train_iter != cv_iter:\n","                  dt_train_prefixes_original = pd.concat([dt_train_prefixes_original, dt_prefixes[cv_train_iter]], axis=0)\n","       \n","        \n","        dt_train_prefixes = dt_test_prefixes_original.copy()\n","        dt_test_prefixes = dt_test_prefixes_original.copy()\n","        \n","        pad_train, pad_test, paddings_train, paddings_test, padded_time, padded_time_test, train_y, test_y = create_data(dt_train_prefixes, dt_test_prefixes)\n","        cat_cols = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['static_cat_cols']\n","        \n","        numerical_columns = cls_encoder_args['dynamic_num_cols']+cls_encoder_args['static_num_cols']\n","        numerical_columns.remove('timesincelastevent')\n","        #create the input layers and embeddings\n","        embeddings= []\n","        input_layers = []\n","        preds_all = []\n","        test_y_all = []\n","        score = 0\n","        dim = 0        \n","        \n","        for i in cat_cols:\n","            cat_weights, index_cat, cat_index, no_cols = create_indexes(i, data)\n","            i = i.replace(':','_')\n","            i = i.replace(' ','_')\n","            input_layer = Input(shape=(cutoff,), name=i)\n","            embedding = Embedding(cat_weights.shape[0],\n","                                          cat_weights.shape[1],\n","                                          weights=[cat_weights],\n","                                          input_length=no_cols,\n","                                        name='embed_'+i)(input_layer)\n","            embeddings.append(embedding)\n","            input_layers.append(input_layer)\n","            dim += cat_weights.shape[1]\n","            \n","        #static input layers\n","        \n","        for j in numerical_columns:\n","            j = j.replace('(','_')\n","            j = j.replace(')','_')\n","            j = j.replace(' ','_')\n","            j = j.replace(':','_')\n","            input_layer = Input(shape=(cutoff,1), name=j)\n","            input_layers.append(input_layer)\n","            embeddings.append(input_layer)\n","            dim +=1\n","         \n","        #Apply dropout on inputs\n","        full_embs = concatenate(embeddings, name='full_embedding')\n","        full_embs = Dropout(args['dropout_rate'])(full_embs)\n","        time_input_layer = Input(shape=(cutoff,1), name='time_input')\n","        input_layers.append(time_input_layer)\n","        time_embs = concatenate([full_embs, time_input_layer], name='allInp')\n","       \n","        dim += 1\n","        l2reg=0.001\n","        \n","        #create the model inputs\n","        model_inputs= []\n","        model_inputs_test= []\n","        for i in range(0,len(paddings_train)):\n","                model_inputs.append(paddings_train[i])\n","            \n","        for i in range(0,len(paddings_test)):\n","                model_inputs_test.append(paddings_test[i])\n","            \n","        for i in range(0,len(pad_train)):\n","                model_inputs.append(reshape_num_data(pad_train[i], cutoff))\n","                 \n","        for i in range(0,len(pad_test)):\n","                model_inputs_test.append(reshape_num_data(pad_test[i], cutoff))\n","                      \n","        model_inputs.append(padded_time)\n","        model_inputs_test.append(padded_time_test)\n","       \n","        if cls_method =='LSTM':\n","                #Compute alpha, visit attention\n","                alpha = Bidirectional(LSTM(args['lstm_size'], return_sequences=True), name='alpha')\n","                alpha_out = alpha(time_embs)\n","                alpha_dense = Dense(1, kernel_regularizer=l2(l2reg))\n","                alpha_out = TimeDistributed(alpha_dense, name='alpha_dense_0')(alpha_out)\n","                alpha_out = Softmax(axis=1, name='alpha_softmax')(alpha_out)\n","                \n","                #Compute beta, codes attention\n","                beta = Bidirectional(LSTM(args['lstm_size'], return_sequences=True),   name='beta')\n","                beta_out = beta(time_embs)\n","                beta_dense = Dense(dim, activation='tanh', kernel_regularizer=l2(l2reg))\n","                beta_out = TimeDistributed(beta_dense, name='beta_dense_0')(beta_out)\n","               \n","                #Compute context vector based on attentions and embeddings\n","                c_t = Multiply()([alpha_out, beta_out, time_embs])\n","                c_t = Lambda(lambda x: backend.sum(x, axis=1))(c_t)\n","\n","                #Make a prediction\n","                contexts = Dropout(args['dropout_rate'])(c_t)\n","                output_layer = Dense(1, activation='sigmoid', name='final_output')(contexts)\n","\n","        elif cls_method =='CNN':\n","                #compute alpha\n","                alpha_dense = Dense(1, kernel_regularizer=l2(l2reg))\n","                alpha_out = TimeDistributed(alpha_dense, name='alpha_dense_0')(embeddings)\n","                alpha_out = Softmax(axis=1, name='alpha_softmax')(alpha_out)\n","\n","                #compute beta           \n","                beta_dense = Dense(dim, activation='tanh', kernel_regularizer=l2(l2reg))\n","                beta_out = TimeDistributed(beta_dense, name='beta_dense_0')(embeddings)  \n","\n","                #conv layer\n","                conv1 = Conv1D(filters=64, kernel_size=5, activation='tanh', input_shape=(maxlen,no_cols))(alpha_out)\n","                conv1 = Conv1D(filters=64, kernel_size=5, activation='tanh', input_shape=(maxlen,no_cols))(beta_out)\n","                \n","                #Compute context vector based on attentions and embeddings\n","                c_t = Multiply()([alpha_out, beta_out, time_embs])\n","                c_t = Lambda(lambda x: backend.sum(x, axis=1))(c_t)\n","\n","                #Make a prediction\n","                contexts = Dropout(args['dropout_rate'])(c_t)\n","                output_layer = Dense(1, activation='sigmoid', name='final_output')(contexts)\n","                \n","        #MODEL\n","        \n","        model = Model(inputs=[input_layers], outputs=output_layer)\n","\n","        if args['optimizer']=='RMSprop':\n","                  opt = RMSprop(learning_rate=args['learning_rate'])\n","        if args['optimizer']=='Nadam':\n","                  opt = Nadam(learning_rate=args['learning_rate'])\n","        if args['optimizer']=='Adam':\n","                  opt = Adam(learning_rate=args['learning_rate'])\n","        if args['optimizer']=='SGD':\n","                  opt = SGD(learning_rate=args['learning_rate'])\n","\n","        model.compile(loss={'final_output':'binary_crossentropy'}, optimizer= opt)\n","\n","        model.summary()\n","           \n","        early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n","        model_checkpoint = ModelCheckpoint('output_files/models/model_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n","        lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n","            \n","      \n","        \n","        result = model.fit(model_inputs,\n","              np.array(train_y),\n","              callbacks=[early_stopping, lr_reducer, model_checkpoint],\n","              validation_split = 0.1,\n","              verbose=2, batch_size=args['batch_size'],\n","              epochs=100)\n","            \n","        # Get the lowest validation loss of the training epochs\n","        validation_loss = np.amin(result.history['val_loss']) \n","        print('Best validation loss of epoch:', validation_loss)\n","        \n","        pred = model.predict(model_inputs_test)\n","        preds_all.extend(pred)\n","        test_y_all.extend(test_y)\n","        score += roc_auc_score(test_y_all, preds_all)\n","        for k, v in args.items():\n","          fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, k, v, score / n_splits))  \n","         \n","        fout_all.write(\"%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, 0))   \n"," \n","        fout_all.flush()\n","        return {'loss': validation_loss, \n","            'status': STATUS_OK, \n","            'model': model, \n","            'args': args}"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVX1WhP2Mvv-"},"source":["# **loop over datasets and classifiers**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IS6puXqYFd8B","outputId":"019a2dac-b36e-419b-e7ad-e61a4e698831"},"source":["for cls_method in classifiers:\n","  for dataset_name in datasets:\n","            dataset_name_csv = res[dataset_name]\n","            print('Dataset:', dataset_name)\n","            print('Classifier', cls_method)\n","            print('Encoding', cls_encoding)\n","            method_name = \"%s_%s\"%(column_selection, cls_encoding)            \n","            # read the data\n","            data = pd.read_csv(dataset_name_csv+'.csv', sep=';')\n","            if dataset_name in ['bpic2011_f1', 'bpic2011_f2', 'bpic2011_f3', 'bpic2011_f4','bpic2015_1_f2','bpic2015_2_f2','bpic2015_3_f2','bpic2015_4_f2','bpic2015_5_f2','sepsis_cases_1','sepsis_cases_2','sepsis_cases_4']:\n","              data['time:timestamp'] = pd.to_datetime(data['time:timestamp'])\n","            if dataset_name in ['bpic2012_accepted', 'bpic2012_cancelled', 'bpic2012_declined']:\n","              data['Complete Timestamp'] = pd.to_datetime(data['Complete Timestamp'])\n","            data['timesincemidnight'] = data['timesincemidnight']/60\n","            data['timesincemidnight'] = round(data['timesincemidnight'],0)\n","            data['timesincecasestart'] = data['timesincecasestart']/60\n","            data['timesincecasestart'] = round(data['timesincecasestart'],0)\n","            data['timesincelastevent'] = data['timesincelastevent']/60\n","            data['timesincelastevent'] = round(data['timesincelastevent'],0)\n","            dataset_manager = DatasetManager(dataset_name)\n","            \n","            cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n","                        'static_cat_cols': dataset_manager.static_cat_cols,\n","                        'static_num_cols': dataset_manager.static_num_cols, \n","                        'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n","                        'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n","                        'fillna': True}\n","\n","            # determine min and max (truncated) prefix lengths\n","            min_prefix_length = 1\n","            if \"traffic_fines\" in dataset_name:\n","                max_prefix_length = 10\n","            elif \"bpic2017\" in dataset_name:\n","                max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n","            else:\n","                max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n","            maxlen = cutoff = max_prefix_length\n","\n","            # split into training and test\n","            train, _ = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n","    \n","    \n","            # prepare chunks for CV\n","            dt_prefixes = []\n","            class_ratios = []\n","            for train_chunk, test_chunk in dataset_manager.get_stratified_split_generator(train, n_splits=n_splits):\n","                class_ratios.append(dataset_manager.get_class_ratio(train_chunk))\n","                # generate data where each prefix is a separate instance\n","                dt_prefixes.append(dataset_manager.generate_prefix_data(test_chunk, min_prefix_length, max_prefix_length))\n","            del train\n","        \n","            # set up search space\n","            if cls_method == \"LSTM\":\n","                space = {'dropout_rate'       : hp.uniform('dropout_rate',0.01,0.3),\n","                'lstm_size'      : scope.int(hp.quniform('units',8,256,8)),\n","                'batch_size' :      scope.int(hp.quniform('batch_size',64,256,8)),\n","               'optimizer': hp.choice('optimizer',['Nadam', 'Adam', 'SGD', 'RMSprop']),\n","               'learning_rate': hp.uniform('learning_rate',0.0001,0.01)\n","                 }\n","            if cls_method == \"CNN\":\n","                space = {'dropout_rate'       : hp.uniform('dropout_rate',0.01,0.3),\n","                'batch_size' :      scope.int(hp.quniform('batch_size',64,256,8)),\n","               'optimizer': hp.choice('optimizer',['Nadam', 'Adam', 'SGD', 'RMSprop']),\n","               'learning_rate': hp.uniform('learning_rate',0.0001,0.01)\n","                 }\n","\n","            # optimize parameters\n","            \n","            trial_nr = 1\n","            trials = Trials()\n","            fout_all = open(os.path.join(params_dir, \"param_optim_all_trials_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name)), \"w\")\n","            rstate = np.random.RandomState(22)\n","            trials = Trials()\n","            best = fmin(create_and_evaluate_model, space, algo=tpe.suggest, max_evals=16, trials=trials, rstate = rstate)\n","            fout_all.close()\n","\n","            # write the best parameters\n","            best_params = hyperopt.space_eval(space, best)\n","            print(best_params)\n","            outfile = os.path.join(params_dir, \"optimal_params_%s_%s_%s.pickle\" % (cls_method, dataset_name, method_name))\n","            textfile = os.path.join(params_dir, \"param_optim_all_trials_%s_%s_%s.txt\" % (cls_method, dataset_name, method_name))\n","            \n","            # write to file\n","            from google.colab import files\n","            with open(textfile, \"w\") as f:\n","              f.write(str(best_params))\n","            files.download(textfile)\n","\n","            from google.colab import files\n","            with open(outfile, \"wb\") as fout:\n","              pickle.dump(best_params, fout)\n","            files.download(outfile)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: bpic2011_f1\n","Classifier LSTM\n","Encoding embeddings\n"]},{"output_type":"stream","name":"stderr","text":["/content/DatasetManager.py:110: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  tmp[\"orig_case_id\"] = tmp[self.case_id_col]\n","/content/DatasetManager.py:111: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  tmp[self.case_id_col] = tmp[self.case_id_col].apply(lambda x: \"%s_%s\"%(x, nr_events))\n","/content/DatasetManager.py:112: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  tmp[\"prefix_nr\"] = nr_events\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," Activity_code (InputLayer)     [(None, 36)]         0           []                               \n"," Producer_code (InputLayer)     [(None, 36)]         0           []                               \n"," Section (InputLayer)           [(None, 36)]         0           []                               \n"," Specialism_code.1 (InputLayer)  [(None, 36)]        0           []                               \n"," group (InputLayer)             [(None, 36)]         0           []                               \n"," Diagnosis (InputLayer)         [(None, 36)]         0           []                               \n"," Treatment_code (InputLayer)    [(None, 36)]         0           []                               \n"," Diagnosis_code (InputLayer)    [(None, 36)]         0           []                               \n"," Specialism_code (InputLayer)   [(None, 36)]         0           []                               \n"," Diagnosis_Treatment_Combinatio  [(None, 36)]        0           []                               \n"," n_ID (InputLayer)                                                                                \n"," embed_Activity_code (Embedding  (None, 36, 195)     38025       ['Activity_code[0][0]']          \n"," )                                                                                                \n"," embed_Producer_code (Embedding  (None, 36, 54)      2916        ['Producer_code[0][0]']          \n"," )                                                                                                \n"," embed_Section (Embedding)      (None, 36, 9)        81          ['Section[0][0]']                \n"," embed_Specialism_code.1 (Embed  (None, 36, 16)      256         ['Specialism_code.1[0][0]']      \n"," ding)                                                                                            \n"," embed_group (Embedding)        (None, 36, 26)       676         ['group[0][0]']                  \n"," embed_Diagnosis (Embedding)    (None, 36, 107)      11449       ['Diagnosis[0][0]']              \n"," embed_Treatment_code (Embeddin  (None, 36, 45)      2025        ['Treatment_code[0][0]']         \n"," g)                                                                                               \n"," embed_Diagnosis_code (Embeddin  (None, 36, 13)      169         ['Diagnosis_code[0][0]']         \n"," g)                                                                                               \n"," embed_Specialism_code (Embeddi  (None, 36, 5)       25          ['Specialism_code[0][0]']        \n"," ng)                                                                                              \n"," embed_Diagnosis_Treatment_Comb  (None, 36, 801)     641601      ['Diagnosis_Treatment_Combination\n"," ination_ID (Embedding)                                          _ID[0][0]']                      \n"," Number_of_executions (InputLay  [(None, 36, 1)]     0           []                               \n"," er)                                                                                              \n"," hour (InputLayer)              [(None, 36, 1)]      0           []                               \n"," weekday (InputLayer)           [(None, 36, 1)]      0           []                               \n"," month (InputLayer)             [(None, 36, 1)]      0           []                               \n"," timesincemidnight (InputLayer)  [(None, 36, 1)]     0           []                               \n"," timesincecasestart (InputLayer  [(None, 36, 1)]     0           []                               \n"," )                                                                                                \n"," event_nr (InputLayer)          [(None, 36, 1)]      0           []                               \n"," open_cases (InputLayer)        [(None, 36, 1)]      0           []                               \n"," Age (InputLayer)               [(None, 36, 1)]      0           []                               \n"," full_embedding (Concatenate)   (None, 36, 1280)     0           ['embed_Activity_code[0][0]',    \n","                                                                  'embed_Producer_code[0][0]',    \n","                                                                  'embed_Section[0][0]',          \n","                                                                  'embed_Specialism_code.1[0][0]',\n","                                                                  'embed_group[0][0]',            \n","                                                                  'embed_Diagnosis[0][0]',        \n","                                                                  'embed_Treatment_code[0][0]',   \n","                                                                  'embed_Diagnosis_code[0][0]',   \n","                                                                  'embed_Specialism_code[0][0]',  \n","                                                                  'embed_Diagnosis_Treatment_Combi\n","                                                                 nation_ID[0][0]',                \n","                                                                  'Number_of_executions[0][0]',   \n","                                                                  'hour[0][0]',                   \n","                                                                  'weekday[0][0]',                \n","                                                                  'month[0][0]',                  \n","                                                                  'timesincemidnight[0][0]',      \n","                                                                  'timesincecasestart[0][0]',     \n","                                                                  'event_nr[0][0]',               \n","                                                                  'open_cases[0][0]',             \n","                                                                  'Age[0][0]']                    \n"," dropout_13 (Dropout)           (None, 36, 1280)     0           ['full_embedding[0][0]']         \n"," time_input (InputLayer)        [(None, 36, 1)]      0           []                               \n"," allInp (Concatenate)           (None, 36, 1281)     0           ['dropout_13[0][0]',             \n","                                                                  'time_input[0][0]']             \n"," alpha (Bidirectional)          (None, 36, 480)      2922240     ['allInp[0][0]']                 \n"," alpha_dense_0 (TimeDistributed  (None, 36, 1)       481         ['alpha[0][0]']                  \n"," )                                                                                                \n"," beta (Bidirectional)           (None, 36, 480)      2922240     ['allInp[0][0]']                 \n"," alpha_softmax (Softmax)        (None, 36, 1)        0           ['alpha_dense_0[0][0]']          \n"," beta_dense_0 (TimeDistributed)  (None, 36, 1281)    616161      ['beta[0][0]']                   \n"," multiply_6 (Multiply)          (None, 36, 1281)     0           ['alpha_softmax[0][0]',          \n","                                                                  'beta_dense_0[0][0]',           \n","                                                                  'allInp[0][0]']                 \n"," lambda_1 (Lambda)              (None, 1281)         0           ['multiply_6[0][0]']             \n"," dropout_14 (Dropout)           (None, 1281)         0           ['lambda_1[0][0]']               \n"," final_output (Dense)           (None, 1)            1282        ['dropout_14[0][0]']             \n","==================================================================================================\n","Total params: 7,159,627\n","Trainable params: 7,159,627\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/100\n","36/36 - 14s - loss: 0.1752 - val_loss: 0.0566 - lr: 0.0051 - 14s/epoch - 392ms/step\n","\n","Epoch 2/100\n","  0%|          | 0/16 [00:30<?, ?it/s, best loss: ?]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  layer_config = serialize_layer_fn(layer)\n","\n"]},{"output_type":"stream","name":"stdout","text":["36/36 - 3s - loss: 0.0330 - val_loss: 0.0521 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 3/100\n","36/36 - 3s - loss: 0.0260 - val_loss: 0.0245 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 4/100\n","36/36 - 3s - loss: 0.0531 - val_loss: 0.0512 - lr: 0.0051 - 3s/epoch - 78ms/step\n","\n","Epoch 5/100\n","36/36 - 3s - loss: 0.0365 - val_loss: 0.0423 - lr: 0.0051 - 3s/epoch - 78ms/step\n","\n","Epoch 6/100\n","36/36 - 3s - loss: 0.0193 - val_loss: 0.0094 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 7/100\n","36/36 - 3s - loss: 0.0334 - val_loss: 0.0423 - lr: 0.0051 - 3s/epoch - 78ms/step\n","\n","Epoch 8/100\n","36/36 - 3s - loss: 0.0241 - val_loss: 0.0193 - lr: 0.0051 - 3s/epoch - 78ms/step\n","\n","Epoch 9/100\n","36/36 - 3s - loss: 0.0116 - val_loss: 0.0352 - lr: 0.0051 - 3s/epoch - 79ms/step\n","\n","Epoch 10/100\n","36/36 - 3s - loss: 0.0109 - val_loss: 0.0912 - lr: 0.0051 - 3s/epoch - 78ms/step\n","\n","Epoch 11/100\n","36/36 - 3s - loss: 0.0616 - val_loss: 0.0309 - lr: 0.0051 - 3s/epoch - 78ms/step\n","\n","Epoch 12/100\n","36/36 - 3s - loss: 0.1217 - val_loss: 0.1031 - lr: 0.0051 - 3s/epoch - 79ms/step\n","\n","Epoch 13/100\n","36/36 - 3s - loss: 0.0509 - val_loss: 0.0202 - lr: 0.0051 - 3s/epoch - 78ms/step\n","\n","Epoch 14/100\n","36/36 - 3s - loss: 0.0363 - val_loss: 0.0559 - lr: 0.0051 - 3s/epoch - 80ms/step\n","\n","Epoch 15/100\n","36/36 - 3s - loss: 0.0307 - val_loss: 0.0082 - lr: 0.0051 - 3s/epoch - 82ms/step\n","\n","Epoch 16/100\n","36/36 - 3s - loss: 0.0118 - val_loss: 0.0106 - lr: 0.0051 - 3s/epoch - 79ms/step\n","\n","Epoch 17/100\n","36/36 - 3s - loss: 0.0140 - val_loss: 0.0056 - lr: 0.0051 - 3s/epoch - 84ms/step\n","\n","Epoch 18/100\n","36/36 - 3s - loss: 0.0511 - val_loss: 0.0321 - lr: 0.0051 - 3s/epoch - 80ms/step\n","\n","Epoch 19/100\n","36/36 - 3s - loss: 0.0186 - val_loss: 0.0045 - lr: 0.0051 - 3s/epoch - 83ms/step\n","\n","Epoch 20/100\n","36/36 - 3s - loss: 0.0151 - val_loss: 0.0071 - lr: 0.0051 - 3s/epoch - 80ms/step\n","\n","Epoch 21/100\n","36/36 - 3s - loss: 0.0161 - val_loss: 0.0047 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 22/100\n","36/36 - 3s - loss: 0.0099 - val_loss: 0.0039 - lr: 0.0051 - 3s/epoch - 84ms/step\n","\n","Epoch 23/100\n","36/36 - 3s - loss: 0.0095 - val_loss: 0.0121 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 24/100\n","36/36 - 3s - loss: 0.0093 - val_loss: 0.0040 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 25/100\n","36/36 - 3s - loss: 0.0119 - val_loss: 0.0031 - lr: 0.0051 - 3s/epoch - 84ms/step\n","\n","Epoch 26/100\n","36/36 - 3s - loss: 0.0086 - val_loss: 0.0044 - lr: 0.0051 - 3s/epoch - 80ms/step\n","\n","Epoch 27/100\n","36/36 - 3s - loss: 0.0109 - val_loss: 0.0031 - lr: 0.0051 - 3s/epoch - 85ms/step\n","\n","Epoch 28/100\n","36/36 - 3s - loss: 0.0079 - val_loss: 0.0049 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 29/100\n","36/36 - 3s - loss: 0.0103 - val_loss: 0.0031 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 30/100\n","36/36 - 3s - loss: 0.0088 - val_loss: 0.0043 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 31/100\n","36/36 - 3s - loss: 0.0078 - val_loss: 0.0044 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 32/100\n","36/36 - 3s - loss: 0.0083 - val_loss: 0.0047 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 33/100\n","36/36 - 3s - loss: 0.0076 - val_loss: 0.0120 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 34/100\n","36/36 - 3s - loss: 0.0095 - val_loss: 0.0151 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 35/100\n","36/36 - 3s - loss: 0.0121 - val_loss: 0.0193 - lr: 0.0051 - 3s/epoch - 81ms/step\n","\n","Epoch 36/100\n","36/36 - 3s - loss: 0.0062 - val_loss: 0.0013 - lr: 0.0026 - 3s/epoch - 84ms/step\n","\n","Epoch 37/100\n","36/36 - 3s - loss: 0.0037 - val_loss: 0.0014 - lr: 0.0026 - 3s/epoch - 81ms/step\n","\n","Epoch 38/100\n","36/36 - 3s - loss: 0.0042 - val_loss: 0.0011 - lr: 0.0026 - 3s/epoch - 85ms/step\n","\n","Epoch 39/100\n","36/36 - 3s - loss: 0.0039 - val_loss: 0.0011 - lr: 0.0026 - 3s/epoch - 82ms/step\n","\n","Epoch 40/100\n","36/36 - 3s - loss: 0.0040 - val_loss: 0.0011 - lr: 0.0026 - 3s/epoch - 82ms/step\n","\n","Epoch 41/100\n","36/36 - 3s - loss: 0.0037 - val_loss: 0.0011 - lr: 0.0026 - 3s/epoch - 82ms/step\n","\n","Epoch 42/100\n","36/36 - 3s - loss: 0.0037 - val_loss: 0.0014 - lr: 0.0026 - 3s/epoch - 82ms/step\n","\n","Epoch 43/100\n","36/36 - 3s - loss: 0.0053 - val_loss: 0.0019 - lr: 0.0026 - 3s/epoch - 82ms/step\n","\n","Epoch 44/100\n","36/36 - 3s - loss: 0.0038 - val_loss: 0.0012 - lr: 0.0026 - 3s/epoch - 81ms/step\n","\n","Epoch 45/100\n","36/36 - 3s - loss: 0.0034 - val_loss: 0.0012 - lr: 0.0026 - 3s/epoch - 82ms/step\n","\n","Epoch 46/100\n","36/36 - 3s - loss: 0.0038 - val_loss: 0.0014 - lr: 0.0026 - 3s/epoch - 82ms/step\n","\n","Epoch 47/100\n","36/36 - 3s - loss: 0.0036 - val_loss: 0.0011 - lr: 0.0026 - 3s/epoch - 82ms/step\n","\n","Epoch 48/100\n","36/36 - 3s - loss: 0.0041 - val_loss: 0.0014 - lr: 0.0026 - 3s/epoch - 83ms/step\n","\n","Epoch 49/100\n","36/36 - 3s - loss: 0.0026 - val_loss: 4.0761e-04 - lr: 0.0013 - 3s/epoch - 87ms/step\n","\n","Epoch 50/100\n","36/36 - 3s - loss: 0.0027 - val_loss: 4.5272e-04 - lr: 0.0013 - 3s/epoch - 83ms/step\n","\n","Epoch 51/100\n","36/36 - 3s - loss: 0.0028 - val_loss: 4.2870e-04 - lr: 0.0013 - 3s/epoch - 83ms/step\n","\n","Epoch 52/100\n","36/36 - 3s - loss: 0.0027 - val_loss: 3.4838e-04 - lr: 0.0013 - 3s/epoch - 87ms/step\n","\n","Epoch 53/100\n","36/36 - 3s - loss: 0.0027 - val_loss: 3.8892e-04 - lr: 0.0013 - 3s/epoch - 83ms/step\n","\n","Epoch 54/100\n","36/36 - 3s - loss: 0.0026 - val_loss: 4.1226e-04 - lr: 0.0013 - 3s/epoch - 83ms/step\n","\n","Epoch 55/100\n","36/36 - 3s - loss: 0.0026 - val_loss: 4.3442e-04 - lr: 0.0013 - 3s/epoch - 83ms/step\n","\n","Epoch 56/100\n","36/36 - 3s - loss: 0.0025 - val_loss: 3.8591e-04 - lr: 0.0013 - 3s/epoch - 83ms/step\n","\n","Epoch 57/100\n","36/36 - 3s - loss: 0.0027 - val_loss: 3.9906e-04 - lr: 0.0013 - 3s/epoch - 83ms/step\n","\n","Epoch 58/100\n","36/36 - 3s - loss: 0.0025 - val_loss: 9.0602e-04 - lr: 0.0013 - 3s/epoch - 83ms/step\n","\n","Epoch 59/100\n","36/36 - 3s - loss: 0.0028 - val_loss: 4.9675e-04 - lr: 0.0013 - 3s/epoch - 83ms/step\n","\n","Epoch 60/100\n","36/36 - 3s - loss: 0.0023 - val_loss: 2.6155e-04 - lr: 6.4077e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 61/100\n","36/36 - 3s - loss: 0.0023 - val_loss: 2.7432e-04 - lr: 6.4077e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 62/100\n","36/36 - 3s - loss: 0.0022 - val_loss: 2.4016e-04 - lr: 6.4077e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 63/100\n","36/36 - 3s - loss: 0.0023 - val_loss: 2.5569e-04 - lr: 6.4077e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 64/100\n","36/36 - 3s - loss: 0.0023 - val_loss: 3.9684e-04 - lr: 6.4077e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 65/100\n","36/36 - 3s - loss: 0.0023 - val_loss: 2.9450e-04 - lr: 6.4077e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 66/100\n","36/36 - 3s - loss: 0.0023 - val_loss: 2.3201e-04 - lr: 6.4077e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 67/100\n","36/36 - 3s - loss: 0.0022 - val_loss: 4.2701e-04 - lr: 6.4077e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 68/100\n","36/36 - 3s - loss: 0.0023 - val_loss: 2.3824e-04 - lr: 6.4077e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 69/100\n","36/36 - 3s - loss: 0.0022 - val_loss: 2.2999e-04 - lr: 6.4077e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 70/100\n","36/36 - 3s - loss: 0.0024 - val_loss: 2.8511e-04 - lr: 6.4077e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 71/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 2.1240e-04 - lr: 3.2038e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 72/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 2.1376e-04 - lr: 3.2038e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 73/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 2.0244e-04 - lr: 3.2038e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 74/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 2.0148e-04 - lr: 3.2038e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 75/100\n","36/36 - 3s - loss: 0.0022 - val_loss: 2.1084e-04 - lr: 3.2038e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 76/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 2.0994e-04 - lr: 3.2038e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 77/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 2.0316e-04 - lr: 3.2038e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 78/100\n","36/36 - 3s - loss: 0.0022 - val_loss: 1.9585e-04 - lr: 3.2038e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 79/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 2.1125e-04 - lr: 3.2038e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 80/100\n","36/36 - 3s - loss: 0.0022 - val_loss: 1.9724e-04 - lr: 3.2038e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 81/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9389e-04 - lr: 1.6019e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 82/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 1.9372e-04 - lr: 1.6019e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 83/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9734e-04 - lr: 1.6019e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 84/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 2.0239e-04 - lr: 1.6019e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 85/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 1.9551e-04 - lr: 1.6019e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 86/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 2.0416e-04 - lr: 1.6019e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 87/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 2.0098e-04 - lr: 1.6019e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 88/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 1.9001e-04 - lr: 1.6019e-04 - 3s/epoch - 87ms/step\n","\n","Epoch 89/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 1.9265e-04 - lr: 1.6019e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 90/100\n","36/36 - 3s - loss: 0.0021 - val_loss: 1.9242e-04 - lr: 1.6019e-04 - 3s/epoch - 83ms/step\n","\n","Epoch 91/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9193e-04 - lr: 8.0096e-05 - 3s/epoch - 83ms/step\n","\n","Epoch 92/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9284e-04 - lr: 8.0096e-05 - 3s/epoch - 84ms/step\n","\n","Epoch 93/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9425e-04 - lr: 8.0096e-05 - 3s/epoch - 85ms/step\n","\n","Epoch 94/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9364e-04 - lr: 8.0096e-05 - 3s/epoch - 86ms/step\n","\n","Epoch 95/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9059e-04 - lr: 8.0096e-05 - 3s/epoch - 86ms/step\n","\n","Epoch 96/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9965e-04 - lr: 8.0096e-05 - 3s/epoch - 86ms/step\n","\n","Epoch 97/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9448e-04 - lr: 8.0096e-05 - 3s/epoch - 87ms/step\n","\n","Epoch 98/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9430e-04 - lr: 8.0096e-05 - 3s/epoch - 86ms/step\n","\n","Epoch 99/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9469e-04 - lr: 8.0096e-05 - 3s/epoch - 86ms/step\n","\n","Epoch 100/100\n","36/36 - 3s - loss: 0.0020 - val_loss: 1.9093e-04 - lr: 8.0096e-05 - 3s/epoch - 86ms/step\n","\n","Best validation loss of epoch:\n","0.0001900062634376809\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," Activity_code (InputLayer)     [(None, 36)]         0           []                               \n"," Producer_code (InputLayer)     [(None, 36)]         0           []                               \n"," Section (InputLayer)           [(None, 36)]         0           []                               \n"," Specialism_code.1 (InputLayer)  [(None, 36)]        0           []                               \n"," group (InputLayer)             [(None, 36)]         0           []                               \n"," Diagnosis (InputLayer)         [(None, 36)]         0           []                               \n"," Treatment_code (InputLayer)    [(None, 36)]         0           []                               \n"," Diagnosis_code (InputLayer)    [(None, 36)]         0           []                               \n"," Specialism_code (InputLayer)   [(None, 36)]         0           []                               \n"," Diagnosis_Treatment_Combinatio  [(None, 36)]        0           []                               \n"," n_ID (InputLayer)                                                                                \n"," embed_Activity_code (Embedding  (None, 36, 195)     38025       ['Activity_code[0][0]']          \n"," )                                                                                                \n"," embed_Producer_code (Embedding  (None, 36, 54)      2916        ['Producer_code[0][0]']          \n"," )                                                                                                \n"," embed_Section (Embedding)      (None, 36, 9)        81          ['Section[0][0]']                \n"," embed_Specialism_code.1 (Embed  (None, 36, 16)      256         ['Specialism_code.1[0][0]']      \n"," ding)                                                                                            \n"," embed_group (Embedding)        (None, 36, 26)       676         ['group[0][0]']                  \n"," embed_Diagnosis (Embedding)    (None, 36, 107)      11449       ['Diagnosis[0][0]']              \n"," embed_Treatment_code (Embeddin  (None, 36, 45)      2025        ['Treatment_code[0][0]']         \n"," g)                                                                                               \n"," embed_Diagnosis_code (Embeddin  (None, 36, 13)      169         ['Diagnosis_code[0][0]']         \n"," g)                                                                                               \n"," embed_Specialism_code (Embeddi  (None, 36, 5)       25          ['Specialism_code[0][0]']        \n"," ng)                                                                                              \n"," embed_Diagnosis_Treatment_Comb  (None, 36, 801)     641601      ['Diagnosis_Treatment_Combination\n"," ination_ID (Embedding)                                          _ID[0][0]']                      \n"," Number_of_executions (InputLay  [(None, 36, 1)]     0           []                               \n"," er)                                                                                              \n"," hour (InputLayer)              [(None, 36, 1)]      0           []                               \n"," weekday (InputLayer)           [(None, 36, 1)]      0           []                               \n"," month (InputLayer)             [(None, 36, 1)]      0           []                               \n"," timesincemidnight (InputLayer)  [(None, 36, 1)]     0           []                               \n"," timesincecasestart (InputLayer  [(None, 36, 1)]     0           []                               \n"," )                                                                                                \n"," event_nr (InputLayer)          [(None, 36, 1)]      0           []                               \n"," open_cases (InputLayer)        [(None, 36, 1)]      0           []                               \n"," Age (InputLayer)               [(None, 36, 1)]      0           []                               \n"," full_embedding (Concatenate)   (None, 36, 1280)     0           ['embed_Activity_code[0][0]',    \n","                                                                  'embed_Producer_code[0][0]',    \n","                                                                  'embed_Section[0][0]',          \n","                                                                  'embed_Specialism_code.1[0][0]',\n","                                                                  'embed_group[0][0]',            \n","                                                                  'embed_Diagnosis[0][0]',        \n","                                                                  'embed_Treatment_code[0][0]',   \n","                                                                  'embed_Diagnosis_code[0][0]',   \n","                                                                  'embed_Specialism_code[0][0]',  \n","                                                                  'embed_Diagnosis_Treatment_Combi\n","                                                                 nation_ID[0][0]',                \n","                                                                  'Number_of_executions[0][0]',   \n","                                                                  'hour[0][0]',                   \n","                                                                  'weekday[0][0]',                \n","                                                                  'month[0][0]',                  \n","                                                                  'timesincemidnight[0][0]',      \n","                                                                  'timesincecasestart[0][0]',     \n","                                                                  'event_nr[0][0]',               \n","                                                                  'open_cases[0][0]',             \n","                                                                  'Age[0][0]']                    \n"," dropout_15 (Dropout)           (None, 36, 1280)     0           ['full_embedding[0][0]']         \n"," time_input (InputLayer)        [(None, 36, 1)]      0           []                               \n"," allInp (Concatenate)           (None, 36, 1281)     0           ['dropout_15[0][0]',             \n","                                                                  'time_input[0][0]']             \n"," alpha (Bidirectional)          (None, 36, 192)      1058304     ['allInp[0][0]']                 \n"," alpha_dense_0 (TimeDistributed  (None, 36, 1)       193         ['alpha[0][0]']                  \n"," )                                                                                                \n"," beta (Bidirectional)           (None, 36, 192)      1058304     ['allInp[0][0]']                 \n"," alpha_softmax (Softmax)        (None, 36, 1)        0           ['alpha_dense_0[0][0]']          \n"," beta_dense_0 (TimeDistributed)  (None, 36, 1281)    247233      ['beta[0][0]']                   \n"," multiply_7 (Multiply)          (None, 36, 1281)     0           ['alpha_softmax[0][0]',          \n","                                                                  'beta_dense_0[0][0]',           \n","                                                                  'allInp[0][0]']                 \n"," lambda_2 (Lambda)              (None, 1281)         0           ['multiply_7[0][0]']             \n"," dropout_16 (Dropout)           (None, 1281)         0           ['lambda_2[0][0]']               \n"," final_output (Dense)           (None, 1)            1282        ['dropout_16[0][0]']             \n","==================================================================================================\n","Total params: 3,062,539\n","Trainable params: 3,062,539\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/100\n","46/46 - 9s - loss: 1.0288 - val_loss: 1.0303 - lr: 0.0018 - 9s/epoch - 190ms/step\n","\n","Epoch 2/100\n","  6%|▋         | 1/16 [05:55<1:22:28, 329.93s/it, best loss: 0.0001900062634376809]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  layer_config = serialize_layer_fn(layer)\n","\n"]},{"output_type":"stream","name":"stdout","text":["46/46 - 2s - loss: 1.0284 - val_loss: 1.0312 - lr: 0.0018 - 2s/epoch - 36ms/step\n","\n","Epoch 3/100\n","46/46 - 2s - loss: 1.0281 - val_loss: 1.0320 - lr: 0.0018 - 2s/epoch - 36ms/step\n","\n","Epoch 4/100\n","46/46 - 2s - loss: 1.0278 - val_loss: 1.0329 - lr: 0.0018 - 2s/epoch - 36ms/step\n","\n","Epoch 5/100\n","46/46 - 2s - loss: 1.0274 - val_loss: 1.0337 - lr: 0.0018 - 2s/epoch - 36ms/step\n","\n","Epoch 6/100\n","46/46 - 2s - loss: 1.0271 - val_loss: 1.0344 - lr: 0.0018 - 2s/epoch - 36ms/step\n","\n","Epoch 7/100\n","46/46 - 2s - loss: 1.0268 - val_loss: 1.0350 - lr: 0.0018 - 2s/epoch - 36ms/step\n","\n","Epoch 8/100\n","46/46 - 2s - loss: 1.0266 - val_loss: 1.0357 - lr: 0.0018 - 2s/epoch - 36ms/step\n","\n","Epoch 9/100\n","46/46 - 2s - loss: 1.0263 - val_loss: 1.0365 - lr: 0.0018 - 2s/epoch - 37ms/step\n","\n","Epoch 10/100\n","46/46 - 2s - loss: 1.0260 - val_loss: 1.0372 - lr: 0.0018 - 2s/epoch - 37ms/step\n","\n","Epoch 11/100\n","46/46 - 2s - loss: 1.0257 - val_loss: 1.0380 - lr: 0.0018 - 2s/epoch - 37ms/step\n","\n","Epoch 12/100\n","46/46 - 2s - loss: 1.0254 - val_loss: 1.0383 - lr: 8.8414e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 13/100\n","46/46 - 2s - loss: 1.0253 - val_loss: 1.0386 - lr: 8.8414e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 14/100\n","46/46 - 2s - loss: 1.0252 - val_loss: 1.0390 - lr: 8.8414e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 15/100\n","46/46 - 2s - loss: 1.0251 - val_loss: 1.0393 - lr: 8.8414e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 16/100\n","46/46 - 2s - loss: 1.0249 - val_loss: 1.0397 - lr: 8.8414e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 17/100\n","46/46 - 2s - loss: 1.0248 - val_loss: 1.0400 - lr: 8.8414e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 18/100\n","46/46 - 2s - loss: 1.0247 - val_loss: 1.0403 - lr: 8.8414e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 19/100\n","46/46 - 2s - loss: 1.0246 - val_loss: 1.0407 - lr: 8.8414e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 20/100\n","46/46 - 2s - loss: 1.0245 - val_loss: 1.0410 - lr: 8.8414e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 21/100\n","46/46 - 2s - loss: 1.0243 - val_loss: 1.0413 - lr: 8.8414e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 22/100\n","46/46 - 2s - loss: 1.0242 - val_loss: 1.0415 - lr: 4.4207e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 23/100\n","46/46 - 2s - loss: 1.0242 - val_loss: 1.0416 - lr: 4.4207e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 24/100\n","46/46 - 2s - loss: 1.0241 - val_loss: 1.0418 - lr: 4.4207e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 25/100\n","46/46 - 2s - loss: 1.0241 - val_loss: 1.0419 - lr: 4.4207e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 26/100\n","46/46 - 2s - loss: 1.0240 - val_loss: 1.0421 - lr: 4.4207e-04 - 2s/epoch - 37ms/step\n","\n","Epoch 27/100\n","46/46 - 2s - loss: 1.0240 - val_loss: 1.0422 - lr: 4.4207e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 28/100\n","46/46 - 2s - loss: 1.0239 - val_loss: 1.0424 - lr: 4.4207e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 29/100\n","46/46 - 2s - loss: 1.0238 - val_loss: 1.0425 - lr: 4.4207e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 30/100\n","46/46 - 2s - loss: 1.0238 - val_loss: 1.0427 - lr: 4.4207e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 31/100\n","46/46 - 2s - loss: 1.0237 - val_loss: 1.0428 - lr: 4.4207e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 32/100\n","46/46 - 2s - loss: 1.0237 - val_loss: 1.0429 - lr: 2.2103e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 33/100\n","46/46 - 2s - loss: 1.0237 - val_loss: 1.0430 - lr: 2.2103e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 34/100\n","46/46 - 2s - loss: 1.0236 - val_loss: 1.0431 - lr: 2.2103e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 35/100\n","46/46 - 2s - loss: 1.0236 - val_loss: 1.0431 - lr: 2.2103e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 36/100\n","46/46 - 2s - loss: 1.0236 - val_loss: 1.0432 - lr: 2.2103e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 37/100\n","46/46 - 2s - loss: 1.0235 - val_loss: 1.0433 - lr: 2.2103e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 38/100\n","46/46 - 2s - loss: 1.0236 - val_loss: 1.0433 - lr: 2.2103e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 39/100\n","46/46 - 2s - loss: 1.0235 - val_loss: 1.0434 - lr: 2.2103e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 40/100\n","46/46 - 2s - loss: 1.0234 - val_loss: 1.0435 - lr: 2.2103e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 41/100\n","46/46 - 2s - loss: 1.0235 - val_loss: 1.0435 - lr: 2.2103e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 42/100\n","46/46 - 2s - loss: 1.0235 - val_loss: 1.0436 - lr: 1.1052e-04 - 2s/epoch - 36ms/step\n","\n","Epoch 43/100\n","46/46 - 2s - loss: 1.0234 - val_loss: 1.0436 - lr: 1.1052e-04 - 2s/epoch - 36ms/step\n","\n","Best validation loss of epoch:\n","1.030342936515808\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," Activity_code (InputLayer)     [(None, 36)]         0           []                               \n"," Producer_code (InputLayer)     [(None, 36)]         0           []                               \n"," Section (InputLayer)           [(None, 36)]         0           []                               \n"," Specialism_code.1 (InputLayer)  [(None, 36)]        0           []                               \n"," group (InputLayer)             [(None, 36)]         0           []                               \n"," Diagnosis (InputLayer)         [(None, 36)]         0           []                               \n"," Treatment_code (InputLayer)    [(None, 36)]         0           []                               \n"," Diagnosis_code (InputLayer)    [(None, 36)]         0           []                               \n"," Specialism_code (InputLayer)   [(None, 36)]         0           []                               \n"," Diagnosis_Treatment_Combinatio  [(None, 36)]        0           []                               \n"," n_ID (InputLayer)                                                                                \n"," embed_Activity_code (Embedding  (None, 36, 195)     38025       ['Activity_code[0][0]']          \n"," )                                                                                                \n"," embed_Producer_code (Embedding  (None, 36, 54)      2916        ['Producer_code[0][0]']          \n"," )                                                                                                \n"," embed_Section (Embedding)      (None, 36, 9)        81          ['Section[0][0]']                \n"," embed_Specialism_code.1 (Embed  (None, 36, 16)      256         ['Specialism_code.1[0][0]']      \n"," ding)                                                                                            \n"," embed_group (Embedding)        (None, 36, 26)       676         ['group[0][0]']                  \n"," embed_Diagnosis (Embedding)    (None, 36, 107)      11449       ['Diagnosis[0][0]']              \n"," embed_Treatment_code (Embeddin  (None, 36, 45)      2025        ['Treatment_code[0][0]']         \n"," g)                                                                                               \n"," embed_Diagnosis_code (Embeddin  (None, 36, 13)      169         ['Diagnosis_code[0][0]']         \n"," g)                                                                                               \n"," embed_Specialism_code (Embeddi  (None, 36, 5)       25          ['Specialism_code[0][0]']        \n"," ng)                                                                                              \n"," embed_Diagnosis_Treatment_Comb  (None, 36, 801)     641601      ['Diagnosis_Treatment_Combination\n"," ination_ID (Embedding)                                          _ID[0][0]']                      \n"," Number_of_executions (InputLay  [(None, 36, 1)]     0           []                               \n"," er)                                                                                              \n"," hour (InputLayer)              [(None, 36, 1)]      0           []                               \n"," weekday (InputLayer)           [(None, 36, 1)]      0           []                               \n"," month (InputLayer)             [(None, 36, 1)]      0           []                               \n"," timesincemidnight (InputLayer)  [(None, 36, 1)]     0           []                               \n"," timesincecasestart (InputLayer  [(None, 36, 1)]     0           []                               \n"," )                                                                                                \n"," event_nr (InputLayer)          [(None, 36, 1)]      0           []                               \n"," open_cases (InputLayer)        [(None, 36, 1)]      0           []                               \n"," Age (InputLayer)               [(None, 36, 1)]      0           []                               \n"," full_embedding (Concatenate)   (None, 36, 1280)     0           ['embed_Activity_code[0][0]',    \n","                                                                  'embed_Producer_code[0][0]',    \n","                                                                  'embed_Section[0][0]',          \n","                                                                  'embed_Specialism_code.1[0][0]',\n","                                                                  'embed_group[0][0]',            \n","                                                                  'embed_Diagnosis[0][0]',        \n","                                                                  'embed_Treatment_code[0][0]',   \n","                                                                  'embed_Diagnosis_code[0][0]',   \n","                                                                  'embed_Specialism_code[0][0]',  \n","                                                                  'embed_Diagnosis_Treatment_Combi\n","                                                                 nation_ID[0][0]',                \n","                                                                  'Number_of_executions[0][0]',   \n","                                                                  'hour[0][0]',                   \n","                                                                  'weekday[0][0]',                \n","                                                                  'month[0][0]',                  \n","                                                                  'timesincemidnight[0][0]',      \n","                                                                  'timesincecasestart[0][0]',     \n","                                                                  'event_nr[0][0]',               \n","                                                                  'open_cases[0][0]',             \n","                                                                  'Age[0][0]']                    \n"," dropout_17 (Dropout)           (None, 36, 1280)     0           ['full_embedding[0][0]']         \n"," time_input (InputLayer)        [(None, 36, 1)]      0           []                               \n"," allInp (Concatenate)           (None, 36, 1281)     0           ['dropout_17[0][0]',             \n","                                                                  'time_input[0][0]']             \n"," alpha (Bidirectional)          (None, 36, 128)      689152      ['allInp[0][0]']                 \n"," alpha_dense_0 (TimeDistributed  (None, 36, 1)       129         ['alpha[0][0]']                  \n"," )                                                                                                \n"," beta (Bidirectional)           (None, 36, 128)      689152      ['allInp[0][0]']                 \n"," alpha_softmax (Softmax)        (None, 36, 1)        0           ['alpha_dense_0[0][0]']          \n"," beta_dense_0 (TimeDistributed)  (None, 36, 1281)    165249      ['beta[0][0]']                   \n"," multiply_8 (Multiply)          (None, 36, 1281)     0           ['alpha_softmax[0][0]',          \n","                                                                  'beta_dense_0[0][0]',           \n","                                                                  'allInp[0][0]']                 \n"," lambda_3 (Lambda)              (None, 1281)         0           ['multiply_8[0][0]']             \n"," dropout_18 (Dropout)           (None, 1281)         0           ['lambda_3[0][0]']               \n"," final_output (Dense)           (None, 1)            1282        ['dropout_18[0][0]']             \n","==================================================================================================\n","Total params: 2,242,187\n","Trainable params: 2,242,187\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/100\n","22/22 - 10s - loss: 0.8942 - val_loss: 0.8738 - lr: 3.2237e-04 - 10s/epoch - 459ms/step\n","\n","Epoch 2/100\n"," 12%|█▎        | 2/16 [07:35<45:11, 193.69s/it, best loss: 0.0001900062634376809]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  layer_config = serialize_layer_fn(layer)\n","\n"]},{"output_type":"stream","name":"stdout","text":["22/22 - 1s - loss: 0.7323 - val_loss: 0.7311 - lr: 3.2237e-04 - 1s/epoch - 63ms/step\n","\n","Epoch 3/100\n","22/22 - 1s - loss: 0.4519 - val_loss: 0.5378 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 4/100\n","22/22 - 1s - loss: 0.2713 - val_loss: 0.4385 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 5/100\n","22/22 - 1s - loss: 0.1839 - val_loss: 0.3848 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 6/100\n","22/22 - 1s - loss: 0.1355 - val_loss: 0.3327 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 7/100\n","22/22 - 1s - loss: 0.1041 - val_loss: 0.3010 - lr: 3.2237e-04 - 1s/epoch - 58ms/step\n","\n","Epoch 8/100\n","22/22 - 1s - loss: 0.0828 - val_loss: 0.2834 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 9/100\n","22/22 - 1s - loss: 0.0684 - val_loss: 0.2311 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 10/100\n","22/22 - 1s - loss: 0.0571 - val_loss: 0.2315 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 11/100\n","22/22 - 1s - loss: 0.0488 - val_loss: 0.2178 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 12/100\n","22/22 - 1s - loss: 0.0421 - val_loss: 0.1982 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 13/100\n","22/22 - 1s - loss: 0.0377 - val_loss: 0.1520 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 14/100\n","22/22 - 1s - loss: 0.0333 - val_loss: 0.1229 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 15/100\n","22/22 - 1s - loss: 0.0311 - val_loss: 0.0992 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 16/100\n","22/22 - 1s - loss: 0.0284 - val_loss: 0.1150 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 17/100\n","22/22 - 1s - loss: 0.0259 - val_loss: 0.0743 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 18/100\n","22/22 - 1s - loss: 0.0247 - val_loss: 0.0554 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 19/100\n","22/22 - 1s - loss: 0.0231 - val_loss: 0.1011 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 20/100\n","22/22 - 1s - loss: 0.0233 - val_loss: 0.0455 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 21/100\n","22/22 - 1s - loss: 0.0212 - val_loss: 0.0396 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 22/100\n","22/22 - 1s - loss: 0.0197 - val_loss: 0.0274 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 23/100\n","22/22 - 1s - loss: 0.0183 - val_loss: 0.0235 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 24/100\n","22/22 - 1s - loss: 0.0179 - val_loss: 0.0263 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 25/100\n","22/22 - 1s - loss: 0.0176 - val_loss: 0.0231 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 26/100\n","22/22 - 1s - loss: 0.0174 - val_loss: 0.0186 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 27/100\n","22/22 - 1s - loss: 0.0162 - val_loss: 0.0171 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 28/100\n","22/22 - 1s - loss: 0.0149 - val_loss: 0.0192 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 29/100\n","22/22 - 1s - loss: 0.0158 - val_loss: 0.0152 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 30/100\n","22/22 - 1s - loss: 0.0141 - val_loss: 0.0142 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 31/100\n","22/22 - 1s - loss: 0.0146 - val_loss: 0.0132 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 32/100\n","22/22 - 1s - loss: 0.0139 - val_loss: 0.0129 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 33/100\n","22/22 - 1s - loss: 0.0134 - val_loss: 0.0122 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 34/100\n","22/22 - 1s - loss: 0.0133 - val_loss: 0.0115 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 35/100\n","22/22 - 1s - loss: 0.0132 - val_loss: 0.0110 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 36/100\n","22/22 - 1s - loss: 0.0128 - val_loss: 0.0106 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 37/100\n","22/22 - 1s - loss: 0.0125 - val_loss: 0.0100 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 38/100\n","22/22 - 1s - loss: 0.0125 - val_loss: 0.0094 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 39/100\n","22/22 - 1s - loss: 0.0121 - val_loss: 0.0093 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 40/100\n","22/22 - 1s - loss: 0.0112 - val_loss: 0.0086 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 41/100\n","22/22 - 1s - loss: 0.0120 - val_loss: 0.0084 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 42/100\n","22/22 - 1s - loss: 0.0117 - val_loss: 0.0084 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 43/100\n","22/22 - 1s - loss: 0.0112 - val_loss: 0.0082 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 44/100\n","22/22 - 1s - loss: 0.0114 - val_loss: 0.0078 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 45/100\n","22/22 - 1s - loss: 0.0109 - val_loss: 0.0078 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 46/100\n","22/22 - 1s - loss: 0.0106 - val_loss: 0.0075 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 47/100\n","22/22 - 1s - loss: 0.0101 - val_loss: 0.0069 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 48/100\n","22/22 - 1s - loss: 0.0101 - val_loss: 0.0066 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 49/100\n","22/22 - 1s - loss: 0.0102 - val_loss: 0.0065 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 50/100\n","22/22 - 1s - loss: 0.0099 - val_loss: 0.0070 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 51/100\n","22/22 - 1s - loss: 0.0098 - val_loss: 0.0063 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 52/100\n","22/22 - 1s - loss: 0.0101 - val_loss: 0.0061 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 53/100\n","22/22 - 1s - loss: 0.0093 - val_loss: 0.0060 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 54/100\n","22/22 - 1s - loss: 0.0092 - val_loss: 0.0060 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 55/100\n","22/22 - 1s - loss: 0.0098 - val_loss: 0.0064 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 56/100\n","22/22 - 1s - loss: 0.0092 - val_loss: 0.0057 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 57/100\n","22/22 - 1s - loss: 0.0091 - val_loss: 0.0056 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 58/100\n","22/22 - 1s - loss: 0.0086 - val_loss: 0.0054 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 59/100\n","22/22 - 1s - loss: 0.0087 - val_loss: 0.0058 - lr: 3.2237e-04 - 1s/epoch - 55ms/step\n","\n","Epoch 60/100\n","22/22 - 1s - loss: 0.0089 - val_loss: 0.0051 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 61/100\n","22/22 - 1s - loss: 0.0082 - val_loss: 0.0051 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 62/100\n","22/22 - 1s - loss: 0.0084 - val_loss: 0.0049 - lr: 3.2237e-04 - 1s/epoch - 61ms/step\n","\n","Epoch 63/100\n","22/22 - 1s - loss: 0.0080 - val_loss: 0.0049 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 64/100\n","22/22 - 1s - loss: 0.0083 - val_loss: 0.0048 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 65/100\n","22/22 - 1s - loss: 0.0080 - val_loss: 0.0047 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 66/100\n","22/22 - 1s - loss: 0.0079 - val_loss: 0.0045 - lr: 3.2237e-04 - 1s/epoch - 61ms/step\n","\n","Epoch 67/100\n","22/22 - 1s - loss: 0.0085 - val_loss: 0.0046 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 68/100\n","22/22 - 1s - loss: 0.0078 - val_loss: 0.0045 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 69/100\n","22/22 - 1s - loss: 0.0078 - val_loss: 0.0044 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 70/100\n","22/22 - 1s - loss: 0.0076 - val_loss: 0.0042 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 71/100\n","22/22 - 1s - loss: 0.0074 - val_loss: 0.0046 - lr: 3.2237e-04 - 1s/epoch - 55ms/step\n","\n","Epoch 72/100\n","22/22 - 1s - loss: 0.0072 - val_loss: 0.0040 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 73/100\n","22/22 - 1s - loss: 0.0077 - val_loss: 0.0042 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 74/100\n","22/22 - 1s - loss: 0.0075 - val_loss: 0.0040 - lr: 3.2237e-04 - 1s/epoch - 61ms/step\n","\n","Epoch 75/100\n","22/22 - 1s - loss: 0.0072 - val_loss: 0.0038 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 76/100\n","22/22 - 1s - loss: 0.0072 - val_loss: 0.0039 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 77/100\n","22/22 - 1s - loss: 0.0071 - val_loss: 0.0043 - lr: 3.2237e-04 - 1s/epoch - 55ms/step\n","\n","Epoch 78/100\n","22/22 - 1s - loss: 0.0072 - val_loss: 0.0037 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 79/100\n","22/22 - 1s - loss: 0.0068 - val_loss: 0.0036 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 80/100\n","22/22 - 1s - loss: 0.0070 - val_loss: 0.0037 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 81/100\n","22/22 - 1s - loss: 0.0069 - val_loss: 0.0035 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 82/100\n","22/22 - 1s - loss: 0.0070 - val_loss: 0.0036 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 83/100\n","22/22 - 1s - loss: 0.0068 - val_loss: 0.0036 - lr: 3.2237e-04 - 1s/epoch - 55ms/step\n","\n","Epoch 84/100\n","22/22 - 1s - loss: 0.0066 - val_loss: 0.0038 - lr: 3.2237e-04 - 1s/epoch - 54ms/step\n","\n","Epoch 85/100\n","22/22 - 1s - loss: 0.0065 - val_loss: 0.0034 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 86/100\n","22/22 - 1s - loss: 0.0069 - val_loss: 0.0033 - lr: 3.2237e-04 - 1s/epoch - 59ms/step\n","\n","Epoch 87/100\n","22/22 - 1s - loss: 0.0067 - val_loss: 0.0033 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 88/100\n","22/22 - 1s - loss: 0.0065 - val_loss: 0.0033 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 89/100\n","22/22 - 1s - loss: 0.0068 - val_loss: 0.0033 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 90/100\n","22/22 - 1s - loss: 0.0065 - val_loss: 0.0033 - lr: 3.2237e-04 - 1s/epoch - 55ms/step\n","\n","Epoch 91/100\n","22/22 - 1s - loss: 0.0065 - val_loss: 0.0031 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 92/100\n","22/22 - 1s - loss: 0.0062 - val_loss: 0.0032 - lr: 3.2237e-04 - 1s/epoch - 55ms/step\n","\n","Epoch 93/100\n","22/22 - 1s - loss: 0.0064 - val_loss: 0.0030 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 94/100\n","22/22 - 1s - loss: 0.0064 - val_loss: 0.0030 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 95/100\n","22/22 - 1s - loss: 0.0062 - val_loss: 0.0030 - lr: 3.2237e-04 - 1s/epoch - 60ms/step\n","\n","Epoch 96/100\n"," 12%|█▎        | 2/16 [09:37<45:11, 193.69s/it, best loss: 0.0001900062634376809]"]}]},{"cell_type":"code","metadata":{"id":"XDB2R89hbtYY"},"source":[""],"execution_count":null,"outputs":[]}]}